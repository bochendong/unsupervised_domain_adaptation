{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.autograd import Function\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReverseLayerF(Function):\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, alpha):\n",
    "        ctx.alpha = alpha\n",
    "\n",
    "        return x.view_as(x)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        output = grad_output.neg() * ctx.alpha\n",
    "\n",
    "        return output, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DANN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DANN, self).__init__()\n",
    "        self.feature = nn.Sequential(\n",
    "                    nn.Conv2d(3, 64, kernel_size=5),\n",
    "                    nn.BatchNorm2d(64),\n",
    "                    nn.MaxPool2d(2),\n",
    "                    nn.ReLU(True),\n",
    "                    nn.Conv2d(64, 50, kernel_size=5),\n",
    "                    nn.BatchNorm2d(50),\n",
    "                    nn.Dropout2d(),\n",
    "                    nn.MaxPool2d(2),\n",
    "                    nn.ReLU(True)\n",
    "                )\n",
    "        self.classifier = nn.Sequential(\n",
    "                    nn.Linear(50 * 4 * 4, 100),\n",
    "                    nn.BatchNorm1d(100),\n",
    "                    nn.ReLU(True),\n",
    "                    nn.Dropout(),\n",
    "                    nn.Linear(100, 100),\n",
    "                    nn.BatchNorm1d(100),\n",
    "                    nn.ReLU(True),\n",
    "                    nn.Linear(100, 10),\n",
    "                    nn.LogSoftmax(dim=1)\n",
    "                )\n",
    "        self.domain_classifier = nn.Sequential(\n",
    "                    nn.Linear(50 * 4 * 4, 100),\n",
    "                    nn.BatchNorm1d(100),\n",
    "                    nn.ReLU(True),\n",
    "                    nn.Linear(100, 2),\n",
    "                    nn.LogSoftmax(dim=1)\n",
    "                )\n",
    "        \n",
    "    def forward(self, input_data, alpha):\n",
    "            input_data = input_data.expand(input_data.data.shape[0], 3, 28, 28)\n",
    "            feature = self.feature(input_data)\n",
    "            feature = feature.view(-1, 50 * 4 * 4)\n",
    "            reverse_feature = ReverseLayerF.apply(feature, alpha)\n",
    "            class_output = self.classifier(feature)\n",
    "            domain_output = self.domain_classifier(reverse_feature)\n",
    "\n",
    "            return class_output, domain_output\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "dann = DANN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import load_mnist_data\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import torch.backends.cudnn as cudnn\n",
    "import random\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, mnist_m_x_train, mnist_m_x_test, y_train, y_test = load_mnist_data.get_numpy_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 28, 28, 3)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.reshape((60000, 3, 28, 28))\n",
    "x_test = x_test.reshape((10000, 3, 28, 28))\n",
    "mnist_m_x_train = mnist_m_x_train.reshape((60000, 3, 28, 28))\n",
    "mnist_m_x_test = mnist_m_x_test.reshape((10000, 3, 28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_X_train = torch.Tensor(x_train)\n",
    "mnist_X_test =  torch.Tensor(x_test)\n",
    "mnist_m_X_train = torch.Tensor(mnist_m_x_train)\n",
    "mnist_m_X_test = torch.Tensor(mnist_m_x_test)\n",
    "train_label = torch.Tensor(y_train)\n",
    "test_label = torch.Tensor(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_transform_source = transforms.Compose([\n",
    "    transforms.Resize(28),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=(0.1307,), std=(0.3081,))\n",
    "])\n",
    "\n",
    "img_transform_target = transforms.Compose([\n",
    "    transforms.Resize(28),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = {}\n",
    "if (torch.cuda.is_available()):\n",
    "    kwargs = {'num_workers': 2, 'pin_memory': True}\n",
    "\n",
    "mnist_dataset = TensorDataset(mnist_X_train,train_label)\n",
    "dataloader_source = DataLoader(mnist_dataset, batch_size=128, **kwargs)\n",
    "\n",
    "mnist_m_dataset = TensorDataset(mnist_m_X_train,train_label)\n",
    "dataloader_target = DataLoader(mnist_m_dataset, batch_size=128, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-3\n",
    "cuda = False\n",
    "n_epoch = 100\n",
    "cudnn.benchmark = True\n",
    "\n",
    "my_net = DANN()\n",
    "optimizer = optim.Adam(my_net.parameters(), lr=lr)\n",
    "\n",
    "loss_class = torch.nn.NLLLoss()\n",
    "loss_domain = torch.nn.NLLLoss()\n",
    "\n",
    "if cuda:\n",
    "    my_net = my_net.cuda()\n",
    "    loss_class = loss_class.cuda()\n",
    "    loss_domain = loss_domain.cuda()\n",
    "\n",
    "for p in my_net.parameters():\n",
    "    p.requires_grad = True\n",
    "\n",
    "\n",
    "loss_class = torch.nn.NLLLoss()\n",
    "loss_domain = torch.nn.NLLLoss()\n",
    "\n",
    "if cuda:\n",
    "    my_net = my_net.cuda()\n",
    "    loss_class = loss_class.cuda()\n",
    "    loss_domain = loss_domain.cuda()\n",
    "\n",
    "for p in my_net.parameters():\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-2.1139, -1.9446, -2.6022, -2.1584, -2.5155, -2.0871, -2.6167, -2.5285,\n",
      "        -2.2533, -2.4789], grad_fn=<SelectBackward0>)\n",
      "tensor(5.)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "expected scalar type Long but found Float",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\55366\\Documents\\GitHub\\unsupervised_domain_adaptation\\dann.ipynb Cell 13'\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/55366/Documents/GitHub/unsupervised_domain_adaptation/dann.ipynb#ch0000022?line=30'>31</a>\u001b[0m \u001b[39mprint\u001b[39m(class_output[\u001b[39m0\u001b[39m])\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/55366/Documents/GitHub/unsupervised_domain_adaptation/dann.ipynb#ch0000022?line=31'>32</a>\u001b[0m \u001b[39mprint\u001b[39m(s_label[\u001b[39m0\u001b[39m])\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/55366/Documents/GitHub/unsupervised_domain_adaptation/dann.ipynb#ch0000022?line=33'>34</a>\u001b[0m err_s_label \u001b[39m=\u001b[39m loss_class(class_output, s_label)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/55366/Documents/GitHub/unsupervised_domain_adaptation/dann.ipynb#ch0000022?line=34'>35</a>\u001b[0m err_s_domain \u001b[39m=\u001b[39m loss_domain(domain_output, domain_label)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/55366/Documents/GitHub/unsupervised_domain_adaptation/dann.ipynb#ch0000022?line=36'>37</a>\u001b[0m \u001b[39m# training model using target data\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ml\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/55366/anaconda3/envs/ml/lib/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/55366/anaconda3/envs/ml/lib/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/55366/anaconda3/envs/ml/lib/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   <a href='file:///c%3A/Users/55366/anaconda3/envs/ml/lib/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> <a href='file:///c%3A/Users/55366/anaconda3/envs/ml/lib/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   <a href='file:///c%3A/Users/55366/anaconda3/envs/ml/lib/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/55366/anaconda3/envs/ml/lib/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ml\\lib\\site-packages\\torch\\nn\\modules\\loss.py:211\u001b[0m, in \u001b[0;36mNLLLoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/55366/anaconda3/envs/ml/lib/site-packages/torch/nn/modules/loss.py?line=209'>210</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor, target: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> <a href='file:///c%3A/Users/55366/anaconda3/envs/ml/lib/site-packages/torch/nn/modules/loss.py?line=210'>211</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mnll_loss(\u001b[39minput\u001b[39;49m, target, weight\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, ignore_index\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mignore_index, reduction\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreduction)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ml\\lib\\site-packages\\torch\\nn\\functional.py:2671\u001b[0m, in \u001b[0;36mnll_loss\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/55366/anaconda3/envs/ml/lib/site-packages/torch/nn/functional.py?line=2668'>2669</a>\u001b[0m \u001b[39mif\u001b[39;00m size_average \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m reduce \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   <a href='file:///c%3A/Users/55366/anaconda3/envs/ml/lib/site-packages/torch/nn/functional.py?line=2669'>2670</a>\u001b[0m     reduction \u001b[39m=\u001b[39m _Reduction\u001b[39m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[1;32m-> <a href='file:///c%3A/Users/55366/anaconda3/envs/ml/lib/site-packages/torch/nn/functional.py?line=2670'>2671</a>\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_nn\u001b[39m.\u001b[39;49mnll_loss_nd(\u001b[39minput\u001b[39;49m, target, weight, _Reduction\u001b[39m.\u001b[39;49mget_enum(reduction), ignore_index)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: expected scalar type Long but found Float"
     ]
    }
   ],
   "source": [
    "# training\n",
    "best_accu_t = 0.0\n",
    "for epoch in range(n_epoch):\n",
    "\n",
    "    len_dataloader = min(len(dataloader_source), len(dataloader_target))\n",
    "    data_source_iter = iter(dataloader_source)\n",
    "    data_target_iter = iter(dataloader_target)\n",
    "\n",
    "    for i in range(len_dataloader):\n",
    "\n",
    "        p = float(i + epoch * len_dataloader) / n_epoch / len_dataloader\n",
    "        alpha = 2. / (1. + np.exp(-10 * p)) - 1\n",
    "\n",
    "        # training model using source data\n",
    "        data_source = data_source_iter.next()\n",
    "        s_img, s_label = data_source\n",
    "\n",
    "        my_net.zero_grad()\n",
    "        batch_size = len(s_label)\n",
    "\n",
    "        domain_label = torch.zeros(batch_size).long()\n",
    "\n",
    "        if cuda:\n",
    "            s_img = s_img.cuda()\n",
    "            s_label = s_label.cuda()\n",
    "            domain_label = domain_label.cuda()\n",
    "\n",
    "        class_output, domain_output = my_net.forward(input_data=s_img, alpha=alpha)\n",
    "\n",
    "        class_output = class_output.type(torch.LongTensor)\n",
    "        s_label = s_label.type(torch.LongTensor)\n",
    "\n",
    "        print(class_output[0])\n",
    "        print(s_label[0])\n",
    "\n",
    "        err_s_label = loss_class(class_output, s_label)\n",
    "        err_s_domain = loss_domain(domain_output, domain_label)\n",
    "\n",
    "        # training model using target data\n",
    "        data_target = data_target_iter.next()\n",
    "        t_img, _ = data_target\n",
    "\n",
    "        batch_size = len(t_img)\n",
    "\n",
    "        domain_label = torch.ones(batch_size).long()\n",
    "\n",
    "        if cuda:\n",
    "            t_img = t_img.cuda()\n",
    "            domain_label = domain_label.cuda()\n",
    "\n",
    "        _, domain_output = my_net.forward(input_data=t_img, alpha=alpha)\n",
    "        domain_output.type(torch.LongTensor)\n",
    "\n",
    "        err_t_domain = loss_domain(domain_output, domain_label)\n",
    "        err = err_t_domain + err_s_domain + err_s_label\n",
    "        err.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        sys.stdout.write('\\r epoch: %d, [iter: %d / all %d], err_s_label: %f, err_s_domain: %f, err_t_domain: %f' \\\n",
    "              % (epoch, i + 1, len_dataloader, err_s_label.data.cpu().numpy(),\n",
    "                 err_s_domain.data.cpu().numpy(), err_t_domain.data.cpu().item()))\n",
    "        sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(trainloader, net, criterion, optimizer, use_cuda=True):\n",
    "    net.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "        batch_size = inputs.size(0)\n",
    "        total += batch_size\n",
    "        if use_cuda:\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        inputs, targets = Variable(inputs), Variable(targets)\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()*batch_size\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        correct += predicted.eq(targets.data).cpu().sum().item()\n",
    "\n",
    "    return train_loss/total, 100 - 100.*correct/total"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f4e5af365abde9565455761093e8e9bc7189c8b509b33061fd0f90eaa1b4d4cb"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('ml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
