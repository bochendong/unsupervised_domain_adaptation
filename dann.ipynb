{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torch.autograd import Function\n",
    "import load_mnist_data\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import torch.backends.cudnn as cudnn\n",
    "import random\n",
    "import os\n",
    "import sys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReverseLayerF(Function):\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, alpha):\n",
    "        ctx.alpha = alpha\n",
    "\n",
    "        return x.view_as(x)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        output = grad_output.neg() * ctx.alpha\n",
    "\n",
    "        return output, None\n",
    "\n",
    "class DANN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DANN, self).__init__()\n",
    "        self.feature = nn.Sequential(\n",
    "                    nn.Conv2d(3, 64, kernel_size=5),\n",
    "                    nn.BatchNorm2d(64),\n",
    "                    nn.MaxPool2d(2),\n",
    "                    nn.ReLU(True),\n",
    "                    nn.Conv2d(64, 50, kernel_size=5),\n",
    "                    nn.BatchNorm2d(50),\n",
    "                    nn.Dropout2d(),\n",
    "                    nn.MaxPool2d(2),\n",
    "                    nn.ReLU(True)\n",
    "                )\n",
    "                \n",
    "        self.avgpool=nn.AdaptiveAvgPool2d((5,5))\n",
    "        self.classifier = nn.Sequential(\n",
    "                    nn.Linear(50 * 4 * 4, 100),\n",
    "                    nn.BatchNorm1d(100),\n",
    "                    nn.ReLU(True),\n",
    "                    nn.Dropout(),\n",
    "                    nn.Linear(100, 100),\n",
    "                    nn.BatchNorm1d(100),\n",
    "                    nn.ReLU(True),\n",
    "                    nn.Linear(100, 10),\n",
    "                )\n",
    "\n",
    "\n",
    "        self.domain_classifier = nn.Sequential(\n",
    "                    nn.Linear(50 * 4 * 4, 100),\n",
    "                    nn.BatchNorm1d(100),\n",
    "                    nn.ReLU(True),\n",
    "                    nn.Linear(100, 2),\n",
    "                )\n",
    "    def forward(self, input_data, alpha):\n",
    "        input_data = input_data.expand(input_data.data.shape[0], 3, 28, 28)\n",
    "        feature = self.feature(input_data)\n",
    "        feature = feature.view(-1, 50 * 4 * 4)\n",
    "        reverse_feature = ReverseLayerF.apply(feature, alpha)\n",
    "        class_output = self.classifier(feature)\n",
    "        domain_output = self.domain_classifier(reverse_feature)\n",
    "\n",
    "        return class_output, domain_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ReverseLayerF(Function):\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, alpha):\n",
    "        ctx.alpha = alpha\n",
    "\n",
    "        return x.view_as(x)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        output = grad_output.neg() * ctx.alpha\n",
    "\n",
    "        return output, None\n",
    "\n",
    "\n",
    "class DANN_with_avg(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DANN, self).__init__()\n",
    "        self.feature = nn.Sequential(\n",
    "                    nn.Conv2d(3, 32, kernel_size=5),\n",
    "                    nn.BatchNorm2d(32),\n",
    "                    nn.MaxPool2d(2),\n",
    "                    nn.ReLU(True),\n",
    "                    nn.Conv2d(32, 48, kernel_size=5),\n",
    "                    nn.BatchNorm2d(48),\n",
    "                    nn.Dropout2d(),\n",
    "                    nn.MaxPool2d(2),\n",
    "                    nn.ReLU(True)\n",
    "                )\n",
    "                \n",
    "        self.avgpool=nn.AdaptiveAvgPool2d((5,5))\n",
    "        self.classifier = nn.Sequential(\n",
    "                    nn.Linear(48*5*5, 100),\n",
    "                    nn.BatchNorm1d(100),\n",
    "                    nn.ReLU(True),\n",
    "                    nn.Dropout(),\n",
    "                    nn.Linear(100, 100),\n",
    "                    nn.BatchNorm1d(100),\n",
    "                    nn.ReLU(True),\n",
    "                    nn.Linear(100, 10),\n",
    "                )\n",
    "\n",
    "\n",
    "        self.domain_classifier = nn.Sequential(\n",
    "                    nn.Linear(48*5*5, 100),\n",
    "                    nn.BatchNorm1d(100),\n",
    "                    nn.ReLU(True),\n",
    "                    nn.Linear(100, 2),\n",
    "                )\n",
    "    def forward(self,x,alpha):\n",
    "        x = x.expand(x.data.shape[0], 3, 28,28)\n",
    "        x=self.feature(x)\n",
    "        x=self.avgpool(x)\n",
    "        x=torch.flatten(x,1)\n",
    "        task_predict=self.classifier(x)\n",
    "        x = ReverseLayerF.apply(x,alpha)\n",
    "        domain_predict=self.domain_classifier(x)\n",
    "        return task_predict,domain_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(source, target, net, criterion, optimizer, epoch, use_cuda=True):\n",
    "    net.train() # Sets the module in training mode.\n",
    "\n",
    "    train_loss = 0\n",
    "    correct_source_label = 0\n",
    "    correct_source_domain = 0\n",
    "    correct_target_label = 0\n",
    "    correct_target_domain = 0\n",
    "    total = 0\n",
    "    batch_size = 128\n",
    "\n",
    "    data_target_iter = iter(target)\n",
    "    len_dataloader = min(len(source), len(target))\n",
    "\n",
    "    for batch_idx, (inputs, source_label) in enumerate(source):\n",
    "\n",
    "        p = float(batch_idx + epoch * len_dataloader) / (200 * len_dataloader)\n",
    "        alpha = 2. / (1. + np.exp(-10 * p)) - 1\n",
    "\n",
    "        batch_size = inputs.size(0)\n",
    "        total += batch_size\n",
    "\n",
    "        # Feed source image to the network\n",
    "        source_label = source_label.type(torch.LongTensor)\n",
    "        domain_label = torch.zeros(batch_size).long()\n",
    "\n",
    "        if use_cuda:\n",
    "            inputs, source_label, domain_label = inputs.cuda(), source_label.cuda(), domain_label.cuda()\n",
    "            \n",
    "        optimizer.zero_grad()\n",
    "        inputs, source_label = Variable(inputs), Variable(source_label)\n",
    "        \n",
    "        class_output, domain_output = net(inputs, alpha)\n",
    "        \n",
    "        _, predicted = torch.max(class_output.data, 1)\n",
    "        correct_source_label += predicted.eq(source_label.data).cpu().sum().item()\n",
    "        _, predicted = torch.max(domain_output.data, 1)\n",
    "        correct_source_domain += predicted.eq(domain_label.data).cpu().sum().item()\n",
    "\n",
    "        loss_s_label = criterion(class_output, source_label)\n",
    "        loss_s_domain = criterion(domain_output, domain_label)\n",
    "\n",
    "        # Feed target image to the network\n",
    "        target_inputs, target_label = data_target_iter.next()\n",
    "        domain_label = torch.ones(batch_size).long()\n",
    "        if use_cuda:\n",
    "            target_inputs, target_label, domain_label = target_inputs.cuda(), target_label.cuda(), domain_label.cuda()\n",
    "        \n",
    "        class_output, domain_output = net(target_inputs, alpha)\n",
    "        loss_t_domain = criterion(domain_output, domain_label)\n",
    "\n",
    "        _, predicted = torch.max(class_output.data, 1)\n",
    "        correct_target_label += predicted.eq(target_label.data).cpu().sum().item()\n",
    "        _, predicted = torch.max(domain_output.data, 1)\n",
    "        correct_target_domain += predicted.eq(domain_label.data).cpu().sum().item()\n",
    "\n",
    "        loss = loss_s_label + loss_s_domain + loss_t_domain\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    return correct_source_label, correct_source_domain, correct_target_label, correct_target_domain, total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader_source, loader_target = load_mnist_data.get_data_loader(1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e: 0, sl: 0.821167, sd: 0.787333, tl: 0.373383, td: 0.757300\n",
      "e: 5, sl: 0.936450, sd: 0.788267, tl: 0.523683, td: 0.780600\n",
      "e: 10, sl: 0.937300, sd: 0.735517, tl: 0.572850, td: 0.726467\n",
      "e: 15, sl: 0.937650, sd: 0.705933, tl: 0.599617, td: 0.695617\n",
      "e: 20, sl: 0.936783, sd: 0.690167, tl: 0.622283, td: 0.680483\n",
      "e: 25, sl: 0.936917, sd: 0.677417, tl: 0.656167, td: 0.668767\n",
      "e: 30, sl: 0.938467, sd: 0.667750, tl: 0.667667, td: 0.661617\n",
      "e: 35, sl: 0.937900, sd: 0.658850, tl: 0.680700, td: 0.652017\n",
      "e: 40, sl: 0.937000, sd: 0.654783, tl: 0.687700, td: 0.646700\n",
      "e: 45, sl: 0.937267, sd: 0.654967, tl: 0.700917, td: 0.644800\n",
      "e: 50, sl: 0.938650, sd: 0.646283, tl: 0.707400, td: 0.635217\n",
      "e: 55, sl: 0.940733, sd: 0.645667, tl: 0.713717, td: 0.638583\n",
      "e: 60, sl: 0.940400, sd: 0.642867, tl: 0.717233, td: 0.633417\n",
      "e: 65, sl: 0.941683, sd: 0.639550, tl: 0.727867, td: 0.633250\n",
      "e: 70, sl: 0.940667, sd: 0.635067, tl: 0.732617, td: 0.625783\n",
      "e: 75, sl: 0.943467, sd: 0.629417, tl: 0.736100, td: 0.619667\n",
      "e: 80, sl: 0.944400, sd: 0.633400, tl: 0.738967, td: 0.627117\n",
      "e: 85, sl: 0.944600, sd: 0.630583, tl: 0.739433, td: 0.619083\n",
      "e: 90, sl: 0.943700, sd: 0.624533, tl: 0.746350, td: 0.621100\n",
      "e: 95, sl: 0.946900, sd: 0.631117, tl: 0.745017, td: 0.625783\n",
      "e: 100, sl: 0.948033, sd: 0.627750, tl: 0.752617, td: 0.618383\n",
      "e: 105, sl: 0.948050, sd: 0.627517, tl: 0.755150, td: 0.619683\n",
      "e: 110, sl: 0.947867, sd: 0.622600, tl: 0.760500, td: 0.615067\n",
      "e: 115, sl: 0.949750, sd: 0.632500, tl: 0.756417, td: 0.624417\n",
      "e: 120, sl: 0.950067, sd: 0.624517, tl: 0.759233, td: 0.612767\n",
      "e: 125, sl: 0.951517, sd: 0.627317, tl: 0.767317, td: 0.616883\n",
      "e: 130, sl: 0.951300, sd: 0.623217, tl: 0.761833, td: 0.614600\n",
      "e: 135, sl: 0.952750, sd: 0.625300, tl: 0.767083, td: 0.615767\n",
      "e: 140, sl: 0.951867, sd: 0.621167, tl: 0.764150, td: 0.611750\n",
      "e: 145, sl: 0.951483, sd: 0.616333, tl: 0.772750, td: 0.610850\n",
      "e: 150, sl: 0.953533, sd: 0.623883, tl: 0.772600, td: 0.612633\n",
      "e: 155, sl: 0.954300, sd: 0.618133, tl: 0.774067, td: 0.609867\n",
      "e: 160, sl: 0.954317, sd: 0.617250, tl: 0.773417, td: 0.610900\n",
      "e: 165, sl: 0.955800, sd: 0.625050, tl: 0.777400, td: 0.617500\n",
      "e: 170, sl: 0.955017, sd: 0.621683, tl: 0.775017, td: 0.611767\n",
      "e: 175, sl: 0.955817, sd: 0.615117, tl: 0.778717, td: 0.607783\n",
      "e: 180, sl: 0.954400, sd: 0.615583, tl: 0.775217, td: 0.612133\n",
      "e: 185, sl: 0.955850, sd: 0.621533, tl: 0.775917, td: 0.609067\n",
      "e: 190, sl: 0.957050, sd: 0.617367, tl: 0.778483, td: 0.607283\n",
      "e: 195, sl: 0.957750, sd: 0.618883, tl: 0.777733, td: 0.609550\n",
      "e: 200, sl: 0.956733, sd: 0.618300, tl: 0.780433, td: 0.607517\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "net = DANN()\n",
    "if (torch.cuda.is_available()):\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "    cudnn.benchmark = True\n",
    "    net.cuda()\n",
    "    criterion = criterion.cuda()\n",
    "    \n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001) \n",
    "\n",
    "for epoch in range(0, 201):\n",
    "    sl, sd, tl, td, total = train(loader_source, loader_target, net, criterion, optimizer, epoch) \n",
    "\n",
    "    if (epoch % 5 == 0):\n",
    "        print(\"e: %d, sl: %f, sd: %f, tl: %f, td: %f\" % (epoch, sl/total, sd/total, tl/total, td/total))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f4e5af365abde9565455761093e8e9bc7189c8b509b33061fd0f90eaa1b4d4cb"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('ml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
