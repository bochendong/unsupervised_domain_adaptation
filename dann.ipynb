{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torch.autograd import Function\n",
    "import load_mnist_data\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReverseLayerF(Function):\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, alpha):\n",
    "        ctx.alpha = alpha\n",
    "\n",
    "        return x.view_as(x)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        output = grad_output.neg() * ctx.alpha\n",
    "\n",
    "        return output, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DANN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DANN, self).__init__()\n",
    "        self.feature = nn.Sequential(\n",
    "                    nn.Conv2d(3, 64, kernel_size=5),\n",
    "                    nn.BatchNorm2d(64),\n",
    "                    nn.MaxPool2d(2),\n",
    "                    nn.ReLU(True),\n",
    "                    nn.Conv2d(64, 50, kernel_size=5),\n",
    "                    nn.BatchNorm2d(50),\n",
    "                    nn.Dropout2d(),\n",
    "                    nn.MaxPool2d(2),\n",
    "                    nn.ReLU(True)\n",
    "                )\n",
    "                \n",
    "        self.avgpool=nn.AdaptiveAvgPool2d((5,5))\n",
    "        self.classifier = nn.Sequential(\n",
    "                    nn.Linear(50 * 4 * 4, 100),\n",
    "                    nn.BatchNorm1d(100),\n",
    "                    nn.ReLU(True),\n",
    "                    nn.Dropout(),\n",
    "                    nn.Linear(100, 100),\n",
    "                    nn.BatchNorm1d(100),\n",
    "                    nn.ReLU(True),\n",
    "                    nn.Linear(100, 10),\n",
    "                )\n",
    "\n",
    "\n",
    "        self.domain_classifier = nn.Sequential(\n",
    "                    nn.Linear(50 * 4 * 4, 100),\n",
    "                    nn.BatchNorm1d(100),\n",
    "                    nn.ReLU(True),\n",
    "                    nn.Linear(100, 2),\n",
    "                )\n",
    "    def forward(self, input_data, alpha):\n",
    "        input_data = input_data.expand(input_data.data.shape[0], 3, 28, 28)\n",
    "        feature = self.feature(input_data)\n",
    "        feature = feature.view(-1, 50 * 4 * 4)\n",
    "        reverse_feature = ReverseLayerF.apply(feature, alpha)\n",
    "        class_output = self.classifier(feature)\n",
    "        domain_output = self.domain_classifier(reverse_feature)\n",
    "\n",
    "        return class_output, domain_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(source, target, net, criterion, optimizer, epoch, use_cuda=True):\n",
    "    net.train() # Sets the module in training mode.\n",
    "\n",
    "    train_loss = 0\n",
    "    correct_source_label = 0\n",
    "    correct_source_domain = 0\n",
    "    correct_target_label = 0\n",
    "    correct_target_domain = 0\n",
    "    total = 0\n",
    "    batch_size = 128\n",
    "\n",
    "    data_target_iter = iter(target)\n",
    "    len_dataloader = min(len(source), len(target))\n",
    "\n",
    "    for batch_idx, (inputs, source_label) in enumerate(source):\n",
    "\n",
    "        p = float(batch_idx + epoch * len_dataloader) / (200 * len_dataloader)\n",
    "        alpha = 2. / (1. + np.exp(-10 * p)) - 1\n",
    "\n",
    "        batch_size = inputs.size(0)\n",
    "        total += batch_size\n",
    "\n",
    "        # Feed source image to the network\n",
    "        source_label = source_label.type(torch.LongTensor)\n",
    "        domain_label = torch.zeros(batch_size).long()\n",
    "\n",
    "        if use_cuda:\n",
    "            inputs, source_label, domain_label = inputs.cuda(), source_label.cuda(), domain_label.cuda()\n",
    "            \n",
    "        optimizer.zero_grad()\n",
    "        inputs, source_label = Variable(inputs), Variable(source_label)\n",
    "        \n",
    "        class_output, domain_output = net(inputs, alpha)\n",
    "        \n",
    "        _, predicted = torch.max(class_output.data, 1)\n",
    "        correct_source_label += predicted.eq(source_label.data).cpu().sum().item()\n",
    "        _, predicted = torch.max(domain_output.data, 1)\n",
    "        correct_source_domain += predicted.eq(domain_label.data).cpu().sum().item()\n",
    "\n",
    "        loss_s_label = criterion(class_output, source_label)\n",
    "        loss_s_domain = criterion(domain_output, domain_label)\n",
    "\n",
    "        # Feed target image to the network\n",
    "        target_inputs, target_label = data_target_iter.next()\n",
    "        domain_label = torch.ones(batch_size).long()\n",
    "        if use_cuda:\n",
    "            target_inputs, target_label, domain_label = target_inputs.cuda(), target_label.cuda(), domain_label.cuda()\n",
    "        \n",
    "        class_output, domain_output = net(target_inputs, alpha)\n",
    "        loss_t_domain = criterion(domain_output, domain_label)\n",
    "\n",
    "        _, predicted = torch.max(class_output.data, 1)\n",
    "        correct_target_label += predicted.eq(target_label.data).cpu().sum().item()\n",
    "        _, predicted = torch.max(domain_output.data, 1)\n",
    "        correct_target_domain += predicted.eq(domain_label.data).cpu().sum().item()\n",
    "\n",
    "        loss = loss_s_label + loss_s_domain + loss_t_domain\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    return correct_source_label, correct_source_domain, correct_target_label, correct_target_domain, total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader_source, loader_target = load_mnist_data.get_data_loader(1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e: 0, sl: 0.825350, sd: 0.819050, tl: 0.357367, td: 0.786783\n",
      "e: 5, sl: 0.945483, sd: 0.866517, tl: 0.489200, td: 0.862967\n",
      "e: 10, sl: 0.947933, sd: 0.792900, tl: 0.541817, td: 0.788483\n",
      "e: 15, sl: 0.948433, sd: 0.766067, tl: 0.580033, td: 0.759033\n",
      "e: 20, sl: 0.950317, sd: 0.740800, tl: 0.602883, td: 0.732117\n",
      "e: 25, sl: 0.946933, sd: 0.725217, tl: 0.623417, td: 0.716683\n",
      "e: 30, sl: 0.948483, sd: 0.703617, tl: 0.622317, td: 0.701867\n",
      "e: 35, sl: 0.947033, sd: 0.694283, tl: 0.636033, td: 0.690783\n",
      "e: 40, sl: 0.947600, sd: 0.688467, tl: 0.650367, td: 0.681950\n",
      "e: 45, sl: 0.947450, sd: 0.680067, tl: 0.657450, td: 0.674250\n",
      "e: 50, sl: 0.946117, sd: 0.673850, tl: 0.667700, td: 0.670467\n",
      "e: 55, sl: 0.947133, sd: 0.666150, tl: 0.679850, td: 0.659467\n",
      "e: 60, sl: 0.947967, sd: 0.663733, tl: 0.679967, td: 0.659450\n",
      "e: 65, sl: 0.947383, sd: 0.657833, tl: 0.683667, td: 0.654183\n",
      "e: 70, sl: 0.947800, sd: 0.650567, tl: 0.687700, td: 0.648967\n",
      "e: 75, sl: 0.948533, sd: 0.651950, tl: 0.689917, td: 0.649033\n",
      "e: 80, sl: 0.948617, sd: 0.647200, tl: 0.700967, td: 0.641483\n",
      "e: 85, sl: 0.948917, sd: 0.648350, tl: 0.700800, td: 0.642250\n",
      "e: 90, sl: 0.949700, sd: 0.639217, tl: 0.709750, td: 0.635067\n",
      "e: 95, sl: 0.949633, sd: 0.637267, tl: 0.709367, td: 0.630900\n",
      "e: 100, sl: 0.950500, sd: 0.634667, tl: 0.707067, td: 0.629117\n",
      "e: 105, sl: 0.951700, sd: 0.636200, tl: 0.712483, td: 0.632700\n",
      "e: 110, sl: 0.951267, sd: 0.634700, tl: 0.712300, td: 0.629967\n",
      "e: 115, sl: 0.951717, sd: 0.634600, tl: 0.720983, td: 0.629483\n",
      "e: 120, sl: 0.950517, sd: 0.631450, tl: 0.721733, td: 0.626783\n",
      "e: 125, sl: 0.952033, sd: 0.631567, tl: 0.723867, td: 0.625533\n",
      "e: 130, sl: 0.953883, sd: 0.628767, tl: 0.725533, td: 0.626050\n",
      "e: 135, sl: 0.951083, sd: 0.629583, tl: 0.723017, td: 0.623617\n",
      "e: 140, sl: 0.952433, sd: 0.626467, tl: 0.726733, td: 0.622917\n",
      "e: 145, sl: 0.954200, sd: 0.630233, tl: 0.731650, td: 0.623000\n",
      "e: 150, sl: 0.954333, sd: 0.625867, tl: 0.730683, td: 0.617633\n",
      "e: 155, sl: 0.955217, sd: 0.627783, tl: 0.740483, td: 0.620050\n",
      "e: 160, sl: 0.954033, sd: 0.623933, tl: 0.738150, td: 0.618417\n",
      "e: 165, sl: 0.955733, sd: 0.628200, tl: 0.737950, td: 0.620100\n",
      "e: 170, sl: 0.955533, sd: 0.618250, tl: 0.741517, td: 0.616467\n",
      "e: 175, sl: 0.956733, sd: 0.629450, tl: 0.737733, td: 0.619683\n",
      "e: 180, sl: 0.956133, sd: 0.624567, tl: 0.743283, td: 0.616267\n",
      "e: 185, sl: 0.956883, sd: 0.621283, tl: 0.738600, td: 0.614150\n",
      "e: 190, sl: 0.957833, sd: 0.621517, tl: 0.744900, td: 0.617500\n",
      "e: 195, sl: 0.957017, sd: 0.625550, tl: 0.747517, td: 0.614050\n",
      "e: 200, sl: 0.959117, sd: 0.622600, tl: 0.748833, td: 0.610867\n",
      "e: 205, sl: 0.959633, sd: 0.621283, tl: 0.742717, td: 0.613833\n",
      "e: 210, sl: 0.956867, sd: 0.618200, tl: 0.744017, td: 0.613567\n",
      "e: 215, sl: 0.957500, sd: 0.620500, tl: 0.744017, td: 0.615283\n",
      "e: 220, sl: 0.958500, sd: 0.618850, tl: 0.746050, td: 0.614683\n",
      "e: 225, sl: 0.956633, sd: 0.613400, tl: 0.748467, td: 0.609100\n",
      "e: 230, sl: 0.959517, sd: 0.616900, tl: 0.751500, td: 0.608300\n",
      "e: 235, sl: 0.959117, sd: 0.619917, tl: 0.752500, td: 0.609667\n",
      "e: 240, sl: 0.960850, sd: 0.620833, tl: 0.756617, td: 0.610733\n",
      "e: 245, sl: 0.960717, sd: 0.621367, tl: 0.753783, td: 0.609317\n",
      "e: 250, sl: 0.959500, sd: 0.620733, tl: 0.755317, td: 0.610783\n",
      "e: 255, sl: 0.960500, sd: 0.621533, tl: 0.753533, td: 0.613150\n",
      "e: 260, sl: 0.961517, sd: 0.616667, tl: 0.754117, td: 0.609933\n",
      "e: 265, sl: 0.960450, sd: 0.614800, tl: 0.758417, td: 0.606933\n",
      "e: 270, sl: 0.961417, sd: 0.617567, tl: 0.763667, td: 0.608767\n",
      "e: 275, sl: 0.961817, sd: 0.618250, tl: 0.761417, td: 0.608633\n",
      "e: 280, sl: 0.962433, sd: 0.615600, tl: 0.764817, td: 0.605317\n",
      "e: 285, sl: 0.962633, sd: 0.615433, tl: 0.762500, td: 0.608517\n",
      "e: 290, sl: 0.963900, sd: 0.611817, tl: 0.764250, td: 0.606850\n",
      "e: 295, sl: 0.963217, sd: 0.616083, tl: 0.759283, td: 0.608867\n",
      "e: 300, sl: 0.961467, sd: 0.615050, tl: 0.759933, td: 0.610083\n",
      "e: 305, sl: 0.963300, sd: 0.613950, tl: 0.763283, td: 0.607100\n",
      "e: 310, sl: 0.963267, sd: 0.614233, tl: 0.762283, td: 0.608350\n",
      "e: 315, sl: 0.962450, sd: 0.615350, tl: 0.763533, td: 0.607983\n",
      "e: 320, sl: 0.962933, sd: 0.614417, tl: 0.769617, td: 0.606700\n",
      "e: 325, sl: 0.963517, sd: 0.615717, tl: 0.769417, td: 0.608583\n",
      "e: 330, sl: 0.964383, sd: 0.616950, tl: 0.769583, td: 0.609250\n",
      "e: 335, sl: 0.963800, sd: 0.612667, tl: 0.768917, td: 0.606117\n",
      "e: 340, sl: 0.963317, sd: 0.611500, tl: 0.765033, td: 0.605867\n",
      "e: 345, sl: 0.965000, sd: 0.616500, tl: 0.771650, td: 0.605533\n",
      "e: 350, sl: 0.965817, sd: 0.610383, tl: 0.769233, td: 0.606817\n",
      "e: 355, sl: 0.965383, sd: 0.615200, tl: 0.776667, td: 0.602417\n",
      "e: 360, sl: 0.965183, sd: 0.615667, tl: 0.772633, td: 0.605600\n",
      "e: 365, sl: 0.965250, sd: 0.608550, tl: 0.778817, td: 0.602717\n",
      "e: 370, sl: 0.965667, sd: 0.612783, tl: 0.771150, td: 0.605700\n",
      "e: 375, sl: 0.965517, sd: 0.612983, tl: 0.775133, td: 0.605533\n",
      "e: 380, sl: 0.964500, sd: 0.612183, tl: 0.774167, td: 0.605950\n",
      "e: 385, sl: 0.966200, sd: 0.612467, tl: 0.771517, td: 0.603067\n",
      "e: 390, sl: 0.965217, sd: 0.611967, tl: 0.777417, td: 0.601667\n",
      "e: 395, sl: 0.965333, sd: 0.610233, tl: 0.775933, td: 0.601833\n",
      "e: 400, sl: 0.966033, sd: 0.616033, tl: 0.779233, td: 0.606500\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "net = DANN()\n",
    "if (torch.cuda.is_available()):\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "    cudnn.benchmark = True\n",
    "    net.cuda()\n",
    "    criterion = criterion.cuda()\n",
    "    \n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001) \n",
    "\n",
    "for epoch in range(0, 201):\n",
    "    sl, sd, tl, td, total = train(loader_source, loader_target, net, criterion, optimizer, epoch) \n",
    "\n",
    "    if (epoch % 10 == 0):\n",
    "        print(\"e: %d, sl: %f, sd: %f, tl: %f, td: %f\" % (epoch, sl/total, sd/total, tl/total, td/total))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f4e5af365abde9565455761093e8e9bc7189c8b509b33061fd0f90eaa1b4d4cb"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('ml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
